# ðŸ”§ INSTALL FIRST (only once):
# pip install biopython tqdm langchain-ollama langchain-huggingface requests beautifulsoup4 lxml

from Bio import Entrez
from tqdm import tqdm
from time import sleep
from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA
from concurrent.futures import ThreadPoolExecutor
import re
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import List, Dict, Tuple


class BiomedicalRAGSystem:
    """
    A comprehensive biomedical RAG system that fetches and analyzes research articles
    from multiple sources (PMC, Europe PMC, PubMed) with source priority handling.
    """
    
    def __init__(self, email: str, api_key: str, model_name: str = "mistral"):
        """Initialize the RAG system with API credentials and configuration."""
        # Configure Entrez
        Entrez.email = email
        Entrez.api_key = api_key
        
        # Initialize models
        self.llm = OllamaLLM(model=model_name)
        # Force GPU usage for embeddings if available
        import torch
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="pritamdeka/S-PubMedBert-MS-MARCO",
            model_kwargs={'device': device}
        )
        print(f"ðŸ”§ Using device: {device.upper()}")
        
        # Configuration
        self.max_results_per_query = {"pubmed": 15, "pmc": 25, "europepmc": 20}
        self.source_priorities = {"PMC-FULLTEXT": 3, "EUROPE-PMC": 2, "PUBMED-ABSTRACT": 1}
        
        # Initialize components
        self.vector_store = None
        self.retriever = None
        self.qa_chain = None
        
        # Query generation prompt
        self.query_prompt = PromptTemplate.from_template("""
You are a biomedical research assistant generating optimal PubMed search queries from clinical cases or model outputs.

Instructions:
- Extract relevant keywords from the case.
- Convert numeric values to qualitative descriptors (e.g., "4.6 cmÂ³" â†’ "large lesion", "7.2 mmHg" â†’ "elevated intracranial pressure").
- Avoid overly specific filters or numerical constraints.
- Each query should target one of: pathophysiology, imaging, treatment.

Input:
{condition}

Return only the queries, one per line, with no explanation.
""")
        
        # RAG analysis prompt
        self.rag_prompt = PromptTemplate.from_template("""
You are a medical expert analyzing clinical cases using evidence from biomedical databases.

IMPORTANT - Source Priority Guidelines:
- **PMC-FULLTEXT**: Complete research articles (HIGHEST PRIORITY - most comprehensive)
- **EUROPE-PMC-ABSTRACT**: Detailed abstracts (MEDIUM PRIORITY - good overview)  
- **PUBMED-ABSTRACT**: Basic abstracts (BASELINE PRIORITY - supplementary info)

When analyzing, prioritize evidence from full-text sources over abstracts.

Context from multiple biomedical databases:
{context}

Question: {question}

Instructions:
- Weight full-text evidence more heavily than abstract-only evidence
- Provide comprehensive analysis based on the highest quality available sources
- Clearly distinguish between findings from full-text vs abstract sources
- Focus on clinical relevance and actionable insights for the specific condition
- If evidence conflicts between sources, note this and explain the source quality difference

Analysis:
""")

    def generate_search_queries(self, condition: str) -> List[str]:
        """Generate focused search queries using LLM."""
        print("\nðŸ“¡ Generating search queries...")
        query_chain = self.query_prompt | self.llm
        query_output = query_chain.invoke({"condition": condition})
        queries = [q.strip() for q in query_output.strip().split('\n') if q.strip()]
        
        print("\nðŸ”Ž Search Queries Generated:")
        for q in queries:
            print(" â€¢", q)
        return queries

    def _is_valid_query(self, query: str) -> bool:
        """Check if query is valid for database search."""
        invalid_terms = ["query", "focus on", ":", "generate"]
        return (len(query.split()) > 2 and 
                not any(term in query.lower() for term in invalid_terms))

    def fetch_pubmed_articles(self, queries: List[str]) -> List[str]:
        """Fetch articles from PubMed database with parallel query processing."""
        def process_single_query(query):
            if not self._is_valid_query(query):
                return []
                
            try:
                handle = Entrez.esearch(db="pubmed", term=query, 
                                      retmax=self.max_results_per_query["pubmed"])
                record = Entrez.read(handle)
                pmids = record["IdList"]

                def fetch_single_article(pmid):
                    try:
                        fetch_handle = Entrez.efetch(db="pubmed", id=pmid, 
                                                   rettype="abstract", retmode="xml")
                        data = Entrez.read(fetch_handle)
                        article = data["PubmedArticle"][0]["MedlineCitation"]["Article"]
                        title = article.get("ArticleTitle", "")
                        abstract = article.get("Abstract", {}).get("AbstractText", [""])[0]
                        return f"**[PUBMED-ABSTRACT]** {title}\n\n{abstract}".strip()
                    except:
                        return None

                with ThreadPoolExecutor(max_workers=8) as executor:
                    results = list(executor.map(fetch_single_article, pmids))
                    return [r for r in results if r]
                    
            except Exception as e:
                print(f"âš ï¸ PubMed query failed: {query} - {e}")
                return []
        
        # Process all queries in parallel
        with ThreadPoolExecutor(max_workers=4) as executor:
            all_results = list(executor.map(process_single_query, queries))
        
        # Flatten results
        articles = []
        for result_list in all_results:
            articles.extend(result_list)
            
        return articles

    def fetch_pmc_articles(self, queries: List[str]) -> List[str]:
        """Fetch full-text articles from PMC database with parallel query processing."""
        def process_single_query(query):
            if len(query.split()) <= 3 or "query" in query.lower():
                return []
                
            try:
                handle = Entrez.esearch(db="pmc", term=query, 
                                      retmax=self.max_results_per_query["pmc"])
                record = Entrez.read(handle)
                pmcids = record["IdList"]

                def fetch_single_pmc_article(pmcid):
                    try:
                        fetch_handle = Entrez.efetch(db="pmc", id=pmcid, 
                                                   rettype="full", retmode="xml")
                        xml_content = fetch_handle.read()
                        root = ET.fromstring(xml_content)
                        
                        # Extract components
                        title_elem = root.find(".//article-title")
                        title = title_elem.text if title_elem is not None else "No Title"
                        
                        abstract_elem = root.find(".//abstract")
                        abstract = ""
                        if abstract_elem is not None:
                            abstract = " ".join([elem.text or "" for elem in abstract_elem.iter() if elem.text])
                        
                        # Extract limited body text
                        body_elems = root.findall(".//body//p")
                        body_text = ""
                        char_count = 0
                        for elem in body_elems[:20]:
                            if elem.text:
                                body_text += elem.text + " "
                                char_count += len(elem.text)
                                if char_count > 3000:
                                    break
                        
                        return f"**[PMC-FULLTEXT]** {title}\n\nAbstract: {abstract}\n\nContent: {body_text}".strip()
                        
                    except:
                        return None

                with ThreadPoolExecutor(max_workers=6) as executor:
                    results = list(executor.map(fetch_single_pmc_article, pmcids))
                    return [r for r in results if r]
                    
            except Exception as e:
                print(f"âš ï¸ PMC query failed: {query} - {e}")
                return []
        
        # Process all queries in parallel  
        with ThreadPoolExecutor(max_workers=2) as executor:  # Lower for PMC rate limits
            all_results = list(executor.map(process_single_query, queries))
        
        # Flatten results
        articles = []
        for result_list in all_results:
            articles.extend(result_list)
            
        return articles

    def fetch_europepmc_articles(self, queries: List[str]) -> List[str]:
        """Fetch articles from Europe PMC database with parallel query processing."""
        base_url = "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
        
        def process_single_query(query):
            if len(query.split()) <= 3 or "query" in query.lower():
                return []
                
            try:
                params = {
                    'query': query,
                    'format': 'json',
                    'resultType': 'core',
                    'pageSize': self.max_results_per_query["europepmc"],
                    'synonym': 'true'
                }
                
                response = requests.get(base_url, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                articles = []
                if 'resultList' in data and 'result' in data['resultList']:
                    for article in data['resultList']['result']:
                        title = article.get('title', 'No Title')
                        abstract = article.get('abstractText', 'No Abstract')
                        
                        if abstract and abstract != 'No Abstract':
                            article_text = f"**[EUROPE-PMC-ABSTRACT]** {title}\n\n{abstract}"
                            articles.append(article_text)
                
                sleep(0.5)  # Rate limiting
                return articles
                
            except Exception as e:
                print(f"âš ï¸ Europe PMC query failed: {query} - {e}")
                return []
        
        # Process all queries in parallel
        with ThreadPoolExecutor(max_workers=1) as executor:
            sleep(0)  # inside process_single_query
            all_results = list(executor.map(process_single_query, queries))
        
        # Flatten results
        articles = []
        for result_list in all_results:
            articles.extend(result_list)
            
        return articles

    def remove_duplicate_articles(self, articles: List[str], similarity_threshold: float = 0.8) -> List[str]:
        """Remove similar articles based on title similarity."""
        unique_articles = []
        seen_titles = []
        
        for article in articles:
            title = article.split('\n')[0].lower()
            title = re.sub(r'\*\*\[.*?\]\*\*\s*', '', title)
            
            is_duplicate = False
            for seen_title in seen_titles:
                common_words = set(title.split()) & set(seen_title.split())
                if (len(common_words) >= 3 and 
                    len(common_words) / max(len(title.split()), len(seen_title.split())) > similarity_threshold):
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_articles.append(article)
                seen_titles.append(title)
        
        return unique_articles

    def build_vector_store(self, articles: List[str]) -> None:
        """Build FAISS vector store from articles with source priority metadata."""
        print("ðŸ§  Embedding articles with source priority...")
        splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)
        documents = []

        for article in articles:
            chunks = splitter.split_text(article)
            for chunk in chunks:
                # Determine source priority
                source_priority = 1  # Default
                for source_tag, priority in self.source_priorities.items():
                    if source_tag in chunk:
                        source_priority = priority
                        break
                        
                documents.append(Document(
                    page_content=chunk,
                    metadata={"source_priority": source_priority}
                ))

        print(f"ðŸ“„ Created {len(documents)} prioritized document chunks.")
        
        print("âš–ï¸ Building FAISS index...")
        self.vector_store = FAISS.from_documents(documents, self.embedding_model)
        self.retriever = self.vector_store.as_retriever(search_kwargs={"k": 12})
        print("âœ… FAISS index built.")

    def setup_qa_chain(self) -> None:
        """Setup the QA chain with custom prompt."""
        if not self.retriever:
            raise ValueError("Vector store must be built before setting up QA chain")
            
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm, 
            retriever=self.retriever,
            chain_type_kwargs={"prompt": self.rag_prompt}
        )

    def analyze_condition(self, condition: str, analysis_query: str) -> Tuple[Dict[str, int], str]:
        """Complete analysis pipeline for a medical condition."""
        # Generate queries
        search_queries = self.generate_search_queries(condition)
        
        # Fetch from all sources in parallel
        print("\nðŸŒ Fetching from sources in parallel (prioritized by content depth)...")
        
        def fetch_source(source_info):
            source_name, fetch_func = source_info
            print(f"\n--- {source_name} ---")
            return fetch_func(search_queries)
        
        # Define sources with their fetch functions
        sources = [
            ("ðŸ¥‡ PMC Full-Text Articles (Priority 1)", self.fetch_pmc_articles),
            ("ðŸ¥ˆ Europe PMC Articles (Priority 2)", self.fetch_europepmc_articles), 
            ("ðŸ¥‰ PubMed Articles (Priority 3)", self.fetch_pubmed_articles)
        ]
        
        # Parallel execution of all sources
        with ThreadPoolExecutor(max_workers=3) as executor:
            results = list(tqdm(executor.map(fetch_source, sources), 
                              total=len(sources), desc="Fetching from all sources"))
        
        pmc_articles, europepmc_articles, pubmed_articles = results
        
        print(f"âœ… Fetched {len(pmc_articles)} PMC full-text articles.")
        print(f"âœ… Fetched {len(europepmc_articles)} Europe PMC articles.")
        print(f"âœ… Fetched {len(pubmed_articles)} PubMed articles.")
        
        # Combine and deduplicate
        all_articles = pmc_articles + europepmc_articles + pubmed_articles
        source_stats = {
            "PMC Full-Text": len(pmc_articles),
            "Europe PMC": len(europepmc_articles),
            "PubMed": len(pubmed_articles),
            "Total": len(all_articles)
        }
        
        print(f"\nðŸ“Š Total articles: {len(all_articles)}")
        for source, count in source_stats.items():
            if source != "Total":
                priority = "Priority 1" if "PMC Full" in source else "Priority 2" if "Europe" in source else "Priority 3"
                print(f"   â€¢ {source}: {count} ({priority})")
        
        print("ðŸ”„ Removing duplicate articles...")
        unique_articles = self.remove_duplicate_articles(all_articles)
        source_stats["Unique"] = len(unique_articles)
        print(f"âœ… After deduplication: {len(unique_articles)} unique articles.")
        
        # Build vector store and QA chain
        self.build_vector_store(unique_articles)
        self.setup_qa_chain()
        
        # Generate analysis
        print("\nðŸ¤– Generating source-prioritized analysis...")
        response = self.qa_chain.invoke(analysis_query)
        analysis_result = response['result'] if isinstance(response, dict) else str(response)
        
        return source_stats, analysis_result

    def save_results(self, condition: str, query: str, source_stats: Dict[str, int], 
                    analysis: str, search_queries: List[str]) -> str:
        """Save analysis results to file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"prioritized_biomed_rag_{timestamp}.txt"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"Prioritized Biomedical RAG Analysis\n")
            f.write(f"Generated: {timestamp}\n")
            f.write(f"Condition: {condition}\n\n")
            f.write(f"Search Queries:\n")
            for q in search_queries:
                f.write(f"â€¢ {q}\n")
            f.write(f"\nSource Summary:\n")
            for source, count in source_stats.items():
                f.write(f"â€¢ {source}: {count}\n")
            f.write(f"\nQuery: {query}\n\n")
            f.write(f"Analysis:\n{analysis}")
        
        print(f"\nðŸ’¾ Results saved to: {filename}")
        return filename


# Example usage
if __name__ == "__main__":
    # Initialize system
    rag_system = BiomedicalRAGSystem(
        email="rohannambiar370@gmail.com",
        api_key="d7d72d403bc1a992b88d7c8befe734858708",
        model_name="mistral"
    )
    
    # Define condition and analysis query
    condition = "tumour of 5.8 cmÂ³ in the temporal lobe with elevated intracranial pressure of 10.2 mmHg"
    analysis_query = "What are the imaging features, treatment approaches, and prognosis of a large lesion in the pons with elevated intracranial pressure?"
    
    # Run complete analysis
    source_stats, analysis = rag_system.analyze_condition(condition, analysis_query)
    
    # Display results
    print("\n===== Prioritized Multi-Source Analysis =====")
    print(analysis)
    
    # Optionally save results
    # rag_system.save_results(condition, analysis_query, source_stats, analysis, search_queries)
