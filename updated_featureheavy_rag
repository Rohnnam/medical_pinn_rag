# üîß INSTALL FIRST (only once):
# pip install biopython tqdm langchain-ollama langchain-huggingface requests beautifulsoup4 lxml

from Bio import Entrez
from tqdm import tqdm
from time import sleep
from langchain.prompts import PromptTemplate
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaLLM
from langchain.chains import RetrievalQA
from concurrent.futures import ThreadPoolExecutor
import re
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from abc import ABC, abstractmethod


@dataclass
class SourceConfig:
    """Configuration for different data sources with priorities."""
    name: str
    priority: int
    max_results: int
    tag: str


@dataclass
class Article:
    """Structured article representation."""
    title: str
    content: str
    source: str
    priority: int
    pmid: Optional[str] = None


class DataSource(ABC):
    """Abstract base class for biomedical data sources."""
    
    def __init__(self, config: SourceConfig):
        self.config = config
    
    @abstractmethod
    def fetch_articles(self, queries: List[str]) -> List[Article]:
        """Fetch articles from the data source."""
        pass
    
    def _is_valid_query(self, query: str) -> bool:
        """Check if query is valid for searching."""
        invalid_terms = ["query", "focus on", ":", "generate"]
        return (len(query.split()) > 2 and 
                not any(term in query.lower() for term in invalid_terms))


class PubMedSource(DataSource):
    """PubMed abstracts data source."""
    
    def fetch_articles(self, queries: List[str]) -> List[Article]:
        articles = []
        
        for query in queries:
            if not self._is_valid_query(query):
                print(f"‚ö†Ô∏è Skipping invalid PubMed query: {query}")
                continue
                
            print(f"\nüî¨ PubMed Query: {query}")
            try:
                handle = Entrez.esearch(db="pubmed", term=query, retmax=self.config.max_results)
                record = Entrez.read(handle)
                pmids = record["IdList"]
                
                with ThreadPoolExecutor(max_workers=6) as executor:
                    results = list(tqdm(
                        executor.map(self._fetch_single_article, pmids),
                        total=len(pmids), 
                        desc=f"Fetching {len(pmids)} PubMed articles"
                    ))
                    articles.extend([r for r in results if r])
                    
            except Exception as e:
                print(f"‚ö†Ô∏è PubMed query failed: {query} - {e}")
                
        return articles
    
    def _fetch_single_article(self, pmid: str) -> Optional[Article]:
        try:
            fetch_handle = Entrez.efetch(db="pubmed", id=pmid, rettype="abstract", retmode="xml")
            data = Entrez.read(fetch_handle)
            article = data["PubmedArticle"][0]["MedlineCitation"]["Article"]
            
            title = article.get("ArticleTitle", "")
            abstract = article.get("Abstract", {}).get("AbstractText", [""])[0]
            
            if abstract:
                content = f"**[{self.config.tag}]** {title}\n\n{abstract}"
                return Article(title, content, self.config.name, self.config.priority, pmid)
                
        except Exception:
            return None


class PMCSource(DataSource):
    """PMC full-text articles data source."""
    
    def fetch_articles(self, queries: List[str]) -> List[Article]:
        articles = []
        
        for query in queries:
            if not self._is_valid_query(query):
                print(f"‚ö†Ô∏è Skipping invalid PMC query: {query}")
                continue
                
            print(f"\nüìö PMC Query: {query}")
            try:
                handle = Entrez.esearch(db="pmc", term=query, retmax=self.config.max_results)
                record = Entrez.read(handle)
                pmcids = record["IdList"]
                
                with ThreadPoolExecutor(max_workers=4) as executor:
                    results = list(tqdm(
                        executor.map(self._fetch_single_article, pmcids),
                        total=len(pmcids),
                        desc=f"Fetching {len(pmcids)} PMC articles"
                    ))
                    articles.extend([r for r in results if r])
                
                sleep(1)  # Rate limiting
                
            except Exception as e:
                print(f"‚ö†Ô∏è PMC query failed: {query} - {e}")
                
        return articles
    
    def _fetch_single_article(self, pmcid: str) -> Optional[Article]:
        try:
            fetch_handle = Entrez.efetch(db="pmc", id=pmcid, rettype="full", retmode="xml")
            xml_content = fetch_handle.read()
            root = ET.fromstring(xml_content)
            
            # Extract components
            title_elem = root.find(".//article-title")
            title = title_elem.text if title_elem is not None else "No Title"
            
            abstract_elem = root.find(".//abstract")
            abstract = ""
            if abstract_elem is not None:
                abstract = " ".join([elem.text or "" for elem in abstract_elem.iter() if elem.text])
            
            # Extract body text (limited)
            body_elems = root.findall(".//body//p")
            body_text = ""
            char_count = 0
            for elem in body_elems[:20]:
                if elem.text:
                    body_text += elem.text + " "
                    char_count += len(elem.text)
                    if char_count > 3000:
                        break
            
            content = f"**[{self.config.tag}]** {title}\n\nAbstract: {abstract}\n\nContent: {body_text}".strip()
            return Article(title, content, self.config.name, self.config.priority, pmcid)
            
        except Exception:
            return None


class EuropePMCSource(DataSource):
    """Europe PMC data source."""
    
    def __init__(self, config: SourceConfig):
        super().__init__(config)
        self.base_url = "https://www.ebi.ac.uk/europepmc/webservices/rest/search"
    
    def fetch_articles(self, queries: List[str]) -> List[Article]:
        articles = []
        
        for query in queries:
            if not self._is_valid_query(query):
                print(f"‚ö†Ô∏è Skipping invalid Europe PMC query: {query}")
                continue
                
            print(f"\nüåç Europe PMC Query: {query}")
            try:
                params = {
                    'query': query,
                    'format': 'json',
                    'resultType': 'core',
                    'pageSize': self.config.max_results,
                    'synonym': 'true'
                }
                
                response = requests.get(self.base_url, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                if 'resultList' in data and 'result' in data['resultList']:
                    for article_data in data['resultList']['result']:
                        title = article_data.get('title', 'No Title')
                        abstract = article_data.get('abstractText', '')
                        
                        if abstract:
                            content = f"**[{self.config.tag}]** {title}\n\n{abstract}"
                            articles.append(Article(title, content, self.config.name, self.config.priority))
                
                sleep(0.5)  # Rate limiting
                
            except Exception as e:
                print(f"‚ö†Ô∏è Europe PMC query failed: {query} - {e}")
                
        return articles


class BiomedicalRAGSystem:
    """Main RAG system for biomedical literature analysis."""
    
    def __init__(self, email: str, api_key: str, model_name: str = "mistral"):
        # Configure Entrez
        Entrez.email = email
        Entrez.api_key = api_key
        
        # Initialize LLM
        self.llm = OllamaLLM(model=model_name)
        
        # Configure data sources with priorities
        self.sources = {
            'pmc': PMCSource(SourceConfig("PMC", 3, 25, "PMC-FULLTEXT")),
            'europepmc': EuropePMCSource(SourceConfig("EuropePMC", 2, 20, "EUROPE-PMC-ABSTRACT")),
            'pubmed': PubMedSource(SourceConfig("PubMed", 1, 15, "PUBMED-ABSTRACT"))
        }
        
        # Initialize components
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)
        self.embeddings = HuggingFaceEmbeddings(model_name="pritamdeka/S-PubMedBert-MS-MARCO")
        self.vector_store = None
        self.qa_chain = None
        
        # Query generation prompt
        self.query_prompt = PromptTemplate.from_template("""
Generate clean PubMed search queries for this medical condition. Extract key terms and create focused queries.

Condition: {condition}

Generate exactly 4 search queries as clean terms only:
- Query 1: Focus on pontine lesions pathophysiology
- Query 2: Focus on brainstem lesions imaging
- Query 3: Focus on intracranial pressure management  
- Query 4: Focus on pontine mass treatment

Return only clean search terms, one per line, no numbering or explanations:
""")
        
        # RAG analysis prompt
        self.rag_prompt = PromptTemplate.from_template("""
You are a medical expert analyzing clinical cases using evidence from biomedical databases.

IMPORTANT - Source Priority Guidelines:
- **PMC-FULLTEXT**: Complete research articles (HIGHEST PRIORITY - most comprehensive)
- **EUROPE-PMC-ABSTRACT**: Detailed abstracts (MEDIUM PRIORITY - good overview)  
- **PUBMED-ABSTRACT**: Basic abstracts (BASELINE PRIORITY - supplementary info)

When analyzing, prioritize evidence from full-text sources over abstracts.

Context from multiple biomedical databases:
{context}

Question: {question}

Instructions:
- Weight full-text evidence more heavily than abstract-only evidence
- Provide comprehensive analysis based on the highest quality available sources
- Clearly distinguish between findings from full-text vs abstract sources
- Focus on clinical relevance and actionable insights for the specific condition
- If evidence conflicts between sources, note this and explain the source quality difference

Analysis:
""")
    
    def generate_search_queries(self, condition: str) -> List[str]:
        """Generate search queries using LLM."""
        print("\nüì° Generating search queries...")
        query_chain = self.query_prompt | self.llm
        query_output = query_chain.invoke({"condition": condition})
        queries = [q.strip() for q in query_output.strip().split('\n') if q.strip()]
        
        print("\nüîé Search Queries Generated:")
        for q in queries:
            print(" ‚Ä¢", q)
        return queries
    
    def fetch_all_articles(self, queries: List[str]) -> List[Article]:
        """Fetch articles from all sources with priority ordering."""
        print("\nüåê Fetching from sources (prioritized by content depth)...")
        
        all_articles = []
        
        # Fetch from sources in priority order
        for source_name, source in sorted(self.sources.items(), 
                                        key=lambda x: x[1].config.priority, reverse=True):
            print(f"\n--- Fetching from {source.config.name} (Priority {source.config.priority}) ---")
            articles = source.fetch_articles(queries)
            all_articles.extend(articles)
            print(f"‚úÖ Fetched {len(articles)} {source.config.name} articles.")
        
        return all_articles
    
    def remove_duplicates(self, articles: List[Article], similarity_threshold: float = 0.8) -> List[Article]:
        """Remove duplicate articles based on title similarity."""
        print("üîÑ Removing duplicate articles...")
        
        unique_articles = []
        seen_titles = []
        
        for article in articles:
            # Clean title for comparison
            title = article.title.lower()
            title = re.sub(r'\*\*\[.*?\]\*\*\s*', '', title)
            
            # Check similarity
            is_duplicate = False
            for seen_title in seen_titles:
                common_words = set(title.split()) & set(seen_title.split())
                similarity = len(common_words) / max(len(title.split()), len(seen_title.split()))
                if len(common_words) >= 3 and similarity > similarity_threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_articles.append(article)
                seen_titles.append(title)
        
        print(f"‚úÖ After deduplication: {len(unique_articles)} unique articles.")
        return unique_articles
    
    def build_vector_store(self, articles: List[Article]) -> None:
        """Build FAISS vector store from articles."""
        print("üß† Embedding articles with source priority...")
        
        documents = []
        for article in articles:
            chunks = self.text_splitter.split_text(article.content)
            for chunk in chunks:
                documents.append(Document(
                    page_content=chunk,
                    metadata={"source_priority": article.priority, "source": article.source}
                ))
        
        print(f"üìÑ Created {len(documents)} prioritized document chunks.")
        
        print("‚öñÔ∏è Building FAISS index...")
        self.vector_store = FAISS.from_documents(documents, self.embeddings)
        print("‚úÖ FAISS index built.")
    
    def setup_qa_chain(self) -> None:
        """Setup the QA chain with retriever."""
        if not self.vector_store:
            raise ValueError("Vector store not built. Call build_vector_store first.")
        
        retriever = self.vector_store.as_retriever(search_kwargs={"k": 12})
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            retriever=retriever,
            chain_type_kwargs={"prompt": self.rag_prompt}
        )
        print("ü§ñ QA chain initialized.")
    
    def analyze_condition(self, condition: str, query: str) -> Tuple[str, List[Document]]:
        """Complete analysis pipeline."""
        # Generate queries
        search_queries = self.generate_search_queries(condition)
        
        # Fetch articles
        articles = self.fetch_all_articles(search_queries)
        
        # Remove duplicates
        unique_articles = self.remove_duplicates(articles)
        
        # Build vector store
        self.build_vector_store(unique_articles)
        
        # Setup QA chain
        self.setup_qa_chain()
        
        # Retrieve relevant documents
        print("\nüîç Retrieving prioritized relevant documents...")
        retriever = self.vector_store.as_retriever(search_kwargs={"k": 12})
        docs = retriever.invoke(query)
        
        print("\n===== Retrieved Prioritized Documents =====")
        for i, doc in enumerate(docs, 1):
            content = doc.page_content
            source_info = self._get_source_info(content)
            print(f"\n--- Doc {i}: {source_info} ---")
            print(f"{content[:400]}...")
        
        # Generate analysis
        print("\nü§ñ Generating source-prioritized analysis...")
        response = self.qa_chain.invoke(query)
        analysis = response['result'] if isinstance(response, dict) else str(response)
        
        return analysis, docs
    
    def _get_source_info(self, content: str) -> str:
        """Extract source information from content."""
        if "**[PMC-FULLTEXT]**" in content:
            return "PMC Full-Text (Priority 1)"
        elif "**[EUROPE-PMC-ABSTRACT]**" in content:
            return "Europe PMC Abstract (Priority 2)"
        elif "**[PUBMED-ABSTRACT]**" in content:
            return "PubMed Abstract (Priority 3)"
        return "Unknown"
    
    def save_results(self, condition: str, query: str, analysis: str, 
                    search_queries: List[str], articles_count: Dict[str, int]) -> str:
        """Save analysis results to file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"prioritized_biomed_rag_{timestamp}.txt"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"Prioritized Biomedical RAG Analysis\n")
            f.write(f"Generated: {timestamp}\n")
            f.write(f"Condition: {condition}\n\n")
            
            f.write(f"Search Queries:\n")
            for q in search_queries:
                f.write(f"‚Ä¢ {q}\n")
            
            f.write(f"\nSource Summary (Priority Order):\n")
            for source_name, count in articles_count.items():
                f.write(f"‚Ä¢ {source_name}: {count}\n")
            
            f.write(f"\nQuery: {query}\n\n")
            f.write(f"Analysis:\n{analysis}")
        
        print(f"\nüíæ Results saved to: {filename}")
        return filename


# === Usage Example ===
if __name__ == "__main__":
    # Configuration
    EMAIL = "rohannambiar370@gmail.com"
    API_KEY = "d7d72d403bc1a992b88d7c8befe734858708"
    CONDITION = "tumour of 5.8 cm¬≥ in the temporal lobe with elevated intracranial pressure of 10.2 mmHg"
    QUERY = "What are the imaging features, treatment approaches, and prognosis of a large lesion in the pons with elevated intracranial pressure?"
    
    # Initialize system
    rag_system = BiomedicalRAGSystem(EMAIL, API_KEY)
    
    # Run analysis
    analysis, retrieved_docs = rag_system.analyze_condition(CONDITION, QUERY)
    
    # Display results
    print("\n===== Prioritized Multi-Source Analysis =====")
    print(analysis)
    
    # Optional: Save results
    # articles_count = {"PMC": 10, "EuropePMC": 15, "PubMed": 20}  # Example counts
    # rag_system.save_results(CONDITION, QUERY, analysis, [], articles_count)
